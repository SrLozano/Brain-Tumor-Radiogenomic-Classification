{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **RSNA-MICCAI Brain Tumor Radiogenomic Classification**\n\n**Predict the status of a genetic biomarker important for brain cancer treatment**\n\n***Mario Lozano Cortés - Universitat de Barcelona - Universitat Politècnica De Catalunya · Barcelona Tech***\n\nThe overarching goal of this Kaggle challenge is to develop a deep-learning-based tool to enable the automated detection of the presence of MGMT promoter methylation using MRI (magnetic resonance imaging) scans.\n\nA malignant tumour in the brain is a life-threatening condition. Known as **glioblastoma, it's both the most common form of brain cancer in adults and the one with the worst prognosis, with median survival being less than a year. The presence of a specific genetic sequence in the tumour known as MGMT promoter methylation has been shown to be a favourable prognostic factor and a strong predictor of responsiveness to chemotherapy.** \n\nCurrently, genetic analysis of cancer requires surgery to extract a tissue sample. Then it can take several weeks to determine the genetic characterization of the tumour. Depending upon the results and type of initial therapy chosen, subsequent surgery may be necessary. **If an accurate method to predict the genetics of cancer through imaging (i.e., radiogenomics) alone could be developed, this would potentially minimize the number of surgeries and refine the type of therapy required.** The introduction of new and customized treatment strategies before surgery has the potential to improve the management, survival, and prospects of patients with brain cancer.\n\nIn this Kaggle competition we will predict the genetic subtype of glioblastoma using MRI (magnetic resonance imaging) scans to train and test out model to detect the presence of MGMT promoter methylation. \n\nThe organizers provided the participants with two sets of data, training and test, consisting of MRI images. The images are sorted by patient. For each patient, several images of four different scan types (FLAIR, T1w, T1wCE, T2) are available. The training set contains scans of 1010 patients. ","metadata":{}},{"cell_type":"markdown","source":"# **Followed approach: Transfer learning**\n\n**In order to deal with the small number of images found in the given dataset it is decided to use a pre-trained model** previously trained on a large dataset on a large-scale image-classification task. In this way, the training can take advantage of the spatial hierarchy of features learned by the pre-trained model.\n\nThe pre-trained model used in this task is VGG16 on the ImageNet dataset (1.4 million labelled images and 1,000 different classes).\n\nThe way of using the pre-trained model is **feature extraction.** A technique that consists of using the representations learned by the pre-trained model. Hence, **an image is passed through the convolutional base of VGG16 and a new classifier is trained on top of it.** ","metadata":{}},{"cell_type":"markdown","source":"# **Imports and variable declarations**\n\nThis section contains the libraries used in the notebook as well as the variables relating to the parameters of the generated network.","metadata":{}},{"cell_type":"markdown","source":"## **Libraries used**\n\nCode based on Tensorflow & Keras","metadata":{}},{"cell_type":"code","source":"import os # os functionalities\nimport re # regular expressions\nimport warnings # avoid tf warnings\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nfrom skimage import io # image manipulation\nimport seaborn as sns # heatmap visualization\nfrom matplotlib import pyplot as plt # image visualization\nwarnings.filterwarnings('ignore') # ignore tensorflow warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # ignore tensorflow warnings\n\n# Tensorflow & Keras & SkLearn\nimport keras\nimport tensorflow as tf \nfrom tensorflow.keras.metrics import AUC # area under ROC\nfrom keras.applications.vgg16 import VGG16 # pretrained model\nfrom sklearn.model_selection import train_test_split # divide dataset\nfrom tensorflow.keras.preprocessing import image # preprocess a read image\nfrom sklearn.metrics import confusion_matrix # metrics for assesing the model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Model variables**\n\nHyperparameters for the built model","metadata":{}},{"cell_type":"code","source":"# Paths to images\ninput_dir = \"../input/rsna-miccai-png/train\" \ntarget_dir = \"../input/rsna-miccai-png/test\"\n\n# Model variables\nepochs = 50\nbatch_size = 32\nlearning_rate = 0.0005\noptimizer = \"RMSprop\"\ndata_augmentation = False\nloss_function = \"binary_crossentropy\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Data variables**\n\nEach independent case has a dedicated folder identified by a five-digit number. Within each of these “case” folders, there are four sub-folders, each of them corresponding to each of the structural multi-parametric MRI (mpMRI) scans, in PNG format. The exact mpMRI scans included are:\n\n- Fluid Attenuated Inversion Recovery (**FLAIR**)\n- T1-weighted pre-contrast (**T1w**)\n- T1-weighted post-contrast (**T1wCE**)\n- T2-weighted (**T2w**)\n\nMoreover, between the collection of images found for a single patient, the following criteria are considered to select the images to be loaded.\n\n- **central**: Since all the images of the same patient are ordered by their id, the one in the middle is selected\n- **first**: Select the first image to appear\n- **all**: Select all the available images for a given patient","metadata":{}},{"cell_type":"code","source":"patient_mode = \"all\" # all - central - first \nimage_modes = [\"FLAIR\"] # FLAIR - T1w - T1wCE - T2w","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data loading**\n\nThe data loading process consists of reading the paths of each of the images according to the selected criteria, dividing them into training, validation and test sets and creating data generators for each of them.","metadata":{}},{"cell_type":"code","source":"def prepare_data(image_mode, patient_mode):\n    \"\"\"\n    This function prepares the paths to the images and gets the labels for each image\n    :parameter image_mode: Type of the images to be loaded (FLAIR - T1w - T1wCE - T2w)\n    :parameter patient_mode: Criteria for selecting the specific images of a patient that are going to be used\n    :return input_img_paths: List of paths to the images\n    :return labels: List of labels for each image\n    \"\"\"\n\n    # Get path to the training images\n    input_img_paths_patients = sorted([\n        os.path.join(input_dir, fname + \"/\" + image_mode) \n        for fname in os.listdir(input_dir)\n        if os.path.exists(os.path.join(input_dir, fname + \"/\" + image_mode))])\n\n    # Get images id's by a specific criteria\n    input_img_paths = []\n    for patient in input_img_paths_patients:\n        images_id = x = [int(re.findall(r'\\d+', x)[0]) for x in os.listdir(patient)] # Get all image ids\n        '''\n        Images for an specific patient are selected following one of these criteria\n        - central: Since all the images of the same patient are ordered by their id, the one in the middle is seletected\n        - first: Select the first one to appear\n        - all: Select all the available images for a given patient\n        '''\n        if patient_mode == \"central\":  # Get middle image\n            index_central_image = min(images_id) + round((max(images_id) - min(images_id))/2)\n            path = os.path.join(patient, \"Image-\" + str(index_central_image) + \".png\")\n            if os.path.exists(path): input_img_paths.append(path)\n        if patient_mode == \"first\": # Get first image\n            index_central_image = min(images_id) \n            path = os.path.join(patient, \"Image-\" + str(index_central_image) + \".png\")\n            if os.path.exists(path): input_img_paths.append(path)\n        if patient_mode == 'all': # Add all of the available images\n            for image in images_id:\n                path = os.path.join(patient, \"Image-\" + str(image) + \".png\")\n                if os.path.exists(path): input_img_paths.append(path)\n\n    # Read csv labels\n    df = pd.read_csv('../input/rsna-miccai-brain-tumor-radiogenomic-classification/train_labels.csv')\n\n    # Obtain pure labels\n    labels = []\n    for img_path in input_img_paths:\n        labels.append(df.loc[df['BraTS21ID'] == int(re.findall(r'\\d+', img_path)[0])]['MGMT_value'].iloc[0])\n        \n    return input_img_paths, labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_data(input_img_paths, labels, verbose=True):\n    \"\"\"\n    This function splits the data between the training, validation and test sets\n    :parameter input_img_paths: List of paths to the images\n    :parameter labels: List of labels for each image\n    :return train_df: Dataframe containing paths and labels of the training set\n    :return val_df: Dataframe containing paths and labels of the validation set\n    :return test_df: Dataframe containing paths and labels of the test set\n    \"\"\"\n    \n    # Split data into train, validation and test\n    X_train, X_test, y_train, y_test = train_test_split(input_img_paths, labels, test_size=0.1, random_state=42)\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.111, random_state=42) # 0.111*0.9 = 0.1\n\n    # Check amounts of data in each split\n    if verbose:\n        print(f\"Training samples: {str(len(X_train))}\\nValidation samples: {str(len(X_val))}\\nTest samples: {str(len(X_test))}\\n\")\n\n    # Create dataframes for keras flow_from_dataframe\n    data = {'id': X_train, 'label': [str(x) for x in y_train]}\n    train_df = pd.DataFrame(data)\n\n    data = {'id': X_val, 'label': [str(x) for x in y_val]}\n    val_df = pd.DataFrame(data)\n\n    data = {'id': X_test, 'label': [str(x) for x in y_test]}\n    test_df = pd.DataFrame(data)\n    \n    return train_df, val_df, test_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_data_generators(train_df, val_df, test_df, data_augmentation=False):\n    \"\"\"\n    This function loads the dataset from dataset folder and returns the image generators associated\n    :parameter train_df: Dataframe with paths to training images and labels\n    :parameter val_df: Dataframe with paths to validation images and labels\n    :parameter test_df: Dataframe with paths to test images and labels\n    :parameter data_augmentation: Boolean indicating whether to include or not data augmentation\n    :return train_generator: Training generator\n    :return val_generator: Validation generator\n    :return test_generator: Testing generator\n    \"\"\"\n\n    # Create a data generator\n    if data_augmentation:\n        datagen_train = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255.0,\n                                                                        rotation_range=20,\n                                                                        width_shift_range=0.1,\n                                                                        height_shift_range=0.1,\n                                                                        zoom_range=0.2,\n                                                                        horizontal_flip=True)\n    else:\n        datagen_train = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255.0)\n        \n    datagen_val_test = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255.0)\n\n    # Load and iterate training dataset\n    train_generator=datagen_train.flow_from_dataframe(dataframe=train_df, x_col=\"id\", y_col=\"label\", \n                                                      class_mode=\"binary\", batch_size=batch_size, target_size=(224, 224))\n    # Load and iterate validation dataset\n    val_generator=datagen_val_test.flow_from_dataframe(dataframe=val_df, x_col=\"id\", y_col=\"label\", \n                                                       class_mode=\"binary\", batch_size=batch_size, target_size=(224, 224))\n    # Load and iterate test dataset\n    test_generator=datagen_val_test.flow_from_dataframe(dataframe=test_df, x_col=\"id\", y_col=\"label\", \n                                                        class_mode=\"binary\", batch_size=batch_size, target_size=(224, 224))\n\n    return train_generator, val_generator, test_generator","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_dataset(image_mode, patient_mode, data_augmentation):\n    \"\"\"\n    This function loads the dataset for a given image mode\n    :parameter image_mode: Type of the images to be loaded (FLAIR - T1w - T1wCE - T2w)\n    :parameter patient_mode: Criteria for selecting the specific images of a patient that are going to be used\n    :parameter data_augmentation: Boolean indicating whether to include or not data augmentation\n    :return train_generator: Training generator\n    :return val_generator: Validation generator\n    :return test_generator: Testing generator\n    \"\"\"\n    \n    input_img_paths, labels = prepare_data(image_mode, patient_mode)\n    train_df, val_df, test_df = split_data(input_img_paths, labels)\n    train_generator, val_generator, test_generator = get_data_generators(train_df, val_df, test_df, data_augmentation)\n    \n    return train_generator, val_generator, test_generator","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Data visualization**\n\nVisualisation of selected image types with positive and negative examples","metadata":{}},{"cell_type":"code","source":"for visualization in [\"T2w\"]:\n\n    # Load images\n    input_img_paths, labels = prepare_data(visualization, \"central\")\n    train_df, val_df, test_df = split_data(input_img_paths, labels, verbose=False)\n\n    samples = 3 # Number of samples to select of each label\n\n    # samples are obtained for no tumor and tumor\n    selected = pd.concat([train_df[train_df.label.eq('0')].sample(samples), train_df[train_df.label.eq('1')].sample(samples)])\n\n    # Create figure\n    fig = plt.figure(figsize=(20, 10))\n\n    # Setting values to rows and column variables\n    rows = 2\n    columns = samples\n\n    # Plot images\n    for element in range(0, len(selected)):\n        fig.add_subplot(rows, columns, element + 1)\n        plt.imshow(io.imread(list(selected['id'])[element]), cmap='gray')\n        plt.title(f\"Tumor presence: {list(selected['label'])[element]}\")\n        plt.savefig(f'{visualization}_visualization.pdf')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As can be seen, **in addition to the lack of data, one of the great difficulties to be faced in this challenge is the lack of uniformity between the images. For the same type of scan, we find images of the brain taken from multiple perspectives (frontal, transversal and lateral)**. This fact complicates the training process and is therefore an obstacle to be overcome.","metadata":{}},{"cell_type":"markdown","source":"# **Model definition**\n\n**Frozen VGG16 Convolutional Base + Custom Classifier**","metadata":{}},{"cell_type":"code","source":"def define_model(verbose=False):\n    \"\"\"\n    This function defines the convolutional neural network model to be used\n    :return train_generator: Training generator\n    \"\"\"\n    \n    # Free up RAM in case the model definition cells were run multiple times\n    keras.backend.clear_session()\n\n    # Build model\n    vgg16_weight_path = '../input/keras-pretrained-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n    base_model = VGG16(include_top=False, input_shape=(224, 224, 3), weights=vgg16_weight_path)\n    #base_model = VGG16(include_top=False, input_shape=(224, 224, 3), weights='imagenet')\n\n    # Make sure that the base_model is running in inference mode here, by passing `training=False`\n    base_model.trainable = False\n\n    # Define model structure\n    inputs = keras.Input(shape=(224, 224, 3))\n    x = base_model(inputs, training=False)\n    # Convert features of shape `base_model.output_shape[1:]` to vectors\n    x = keras.layers.GlobalAveragePooling2D()(x)\n    # Dense classifier for binary classification\n    x = keras.layers.Dense(128, activation='relu')(x)\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.Dense(32, activation='relu')(x)\n    x = keras.layers.Dense(16, activation='relu')(x)\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n    model = keras.Model(inputs, outputs)\n    if verbose:\n        model.summary()\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Choose optimizer - SGD - RMSprop - Adam\nif optimizer == \"SGD\": \n    fit_optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9, decay=learning_rate/epochs)\nelif optimizer == \"RMSprop\":\n    fit_optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate, decay=learning_rate/epochs)\nelse: \n    fit_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-7, amsgrad=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model training**\n\nIn this section the model is trained with the parameters selected in the *Imports and variable declarations* section.","metadata":{}},{"cell_type":"code","source":"def train_model(model, fit_optimizer, loss_function, epochs, train_generator, val_generator, image_mode):\n    \"\"\"\n    This function trains a given model with the parameters specified the dataset for a given image mode\n    :parameter model: Keras model to be trained \n    :parameter fit_optimizer: Optimizers to compute the moving average and overwrite the model variables at desired time\n    :parameter loss_function: Objective function\n    :parameter epochs: Number of episodes to trained the model\n    :parameter training_generator: Training generator\n    :parameter val_generator: Validation generator\n    :parameter image_mode: Type of the images to be loaded (FLAIR - T1w - T1wCE - T2w)\n    :return model: Keras model trained\n    :return model_history: Model training history \n    \"\"\"\n    \n    # Free up RAM in case the model definition cells were run multiple times\n    keras.backend.clear_session()\n    \n    # Configure the model for training.\n    model.compile(optimizer=fit_optimizer, loss=loss_function, metrics=['accuracy', AUC()])\n\n    # Safe best model and Early Stopping\n    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=3)]\n\n    # Train the model, doing validation at the end of each epoch.\n    model_history = model.fit(train_generator, epochs=epochs, validation_data=val_generator, callbacks=callbacks)\n    \n    return model, model_history","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store models, scores, training histories and test generators into dictionaries\nmodels = {}\nscores = {}\nmodel_histories = {}\ntest_generators = {}\n\n# Train a model for each image_mode available\nfor image_mode in image_modes:\n    print(f\"\\nTraining with: {image_mode} images\\n\")\n    \n    # Prepare and load data\n    train_generator, val_generator, test_generator = load_dataset(image_mode, patient_mode, data_augmentation)\n\n    # Define model\n    model = define_model()\n\n    # Train model\n    model, model_histories[image_mode] = train_model(model, fit_optimizer, loss_function, epochs, train_generator, val_generator, image_mode)\n    models[image_mode] = model\n    \n    # Evaluate the model\n    test_generators[image_mode] = test_generator\n    test_generator.reset()\n    scores[image_mode] = model.evaluate(test_generator, verbose=0)\n    print(f\"\\nLoss: {str(scores[image_mode][0])} \\nAccuracy on test: {str(scores[image_mode][1])}\\nAUC: {str(scores[image_mode][2])}\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model evaluation**\n\nIn this section, the following evaluation metrics are calculated:\n\n- **Accuracy**: Denotes the percent of instances that are accurately classified. This metric calculates the ratio between the amount of adequately classified images and the total number of images in the test set. (Correct predictions / Number of predictions).\n\n- **AUC. Area Under the ROC Curve**: A ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters: True Positive Rate and False Positive Rate. A ROC curve plots TPR vs. FPR at different classification thresholds. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives. AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.\n\nMoreover, the following graphs are generated:\n\n- **Accuracy and loss evolution during training**\n\n- **Confusion matrix**: Table that summarizes the number of correct and incorrect predictions that a classification model made.\n\n*The main metric used in the challenge is AUC an hence, it will be given the most importance.*","metadata":{}},{"cell_type":"code","source":"# Get best model\nbest_model = list(scores.keys())[0]\nfor image_mode in scores:\n    if scores[image_mode][2] > scores[best_model][2]:\n        best_model = image_mode\n        \n# Serialize and save model to json\nmodel_json = models[best_model].to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n\n# Serialize and save weights to HDF5\nmodels[best_model].save_weights(\"model.h5\")\nprint(\"Saved model to disk\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Accuracy plot to show the evolution of the training process of the best model\nplt.figure(figsize=(7, 5), dpi=100)\nplt.plot(model_histories[best_model].history['accuracy'])\nplt.plot(model_histories[best_model].history['val_accuracy'])\nplt.title(f'Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.savefig(f'Brain_Tumor_accuracy.pdf')\nplt.show()\nplt.close()\n\n# Loss plot to show the evolution of the training process of the best model\nplt.figure(figsize=(7, 5), dpi=100)\nplt.plot(model_histories[best_model].history['loss'])\nplt.plot(model_histories[best_model].history['val_loss'])\nplt.title(f'Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.savefig(f'Brain_Tumor_loss.pdf')\nplt.show()\nplt.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The loss is used to know when network training has to stop. Therefore, **attention should be paid to when the generalisation error of the network increases, i.e. when the validation loss stops improving while the training loss continues to improve.**","metadata":{}},{"cell_type":"code","source":"# Evaluate the model\ntest_generators[best_model].reset()\nscore = models[best_model].evaluate(test_generators[best_model], verbose=0)\nprint(f\"Loss: {str(score[0])} \\nAccuracy on test: {str(score[1])}\\nAUC: {str(score[2])}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assign most probable label to predictions\ntest_generators[best_model].reset()\npred = models[best_model].predict(test_generators[best_model], verbose=0)\npredicted_class_indices = np.round(pred)\n\n# Get class labels\ntarget_names = (test_generators[best_model].class_indices).keys()\n\n# Get confusion matrix\nfig, ax = plt.subplots(figsize=(10, 10), dpi=100)\ncf_matrix = confusion_matrix(np.array(test_generators[best_model].classes), predicted_class_indices)\nheatmap = sns.heatmap(cf_matrix/np.sum(cf_matrix), fmt='.2%', annot=True, cmap='Blues', cbar=True, square=False, \n                      annot_kws={\"fontsize\":32}, xticklabels=target_names, yticklabels=target_names)\nfig = heatmap.get_figure()\nplt.savefig(f'Brain_Tumor_confusion_matrix.pdf')\nplt.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Challenge submission**\n\nThis code generates predictions on the public test data of the challenge and generates the submission file.","metadata":{}},{"cell_type":"code","source":"# Get path to the training images\ninput_img_paths_patients = sorted([\n    os.path.join(\"../input/rsna-miccai-png/test\", fname + \"/\" + \"FLAIR\") \n    for fname in os.listdir(\"../input/rsna-miccai-png/test\")\n    if os.path.exists(os.path.join(\"../input/rsna-miccai-png/test\", fname + \"/\" + \"FLAIR\"))])\n\n# Get images id's for central criteria\ninput_img_paths = []\nfor patient in input_img_paths_patients:\n    images_id = x = [int(re.findall(r'\\d+', x)[0]) for x in os.listdir(patient)] # Get all image ids\n    \n    # Get central image for patient\n    index_central_image = min(images_id) + round((max(images_id) - min(images_id))/2)\n    path = os.path.join(patient, \"Image-\" + str(index_central_image) + \".png\")\n    if os.path.exists(path): input_img_paths.append(path)\n\n# Make predictions\nresults = []\nfor img_input in input_img_paths:\n    img = image.load_img(img_input, target_size=(224, 224))\n    img_array = image.img_to_array(img)\n    img_batch = np.expand_dims(img_array, axis=0)\n    img_batch *= 255.0/img_batch.max() \n    results.append(models[best_model].predict(img_batch, verbose=0)[0][0])\n\n# Store predictions in suitable format for the challenge submission\nsubm = pd.read_csv('../input/rsna-miccai-brain-tumor-radiogenomic-classification/sample_submission.csv')\nsubm['MGMT_value'] = results\nsubm.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Results, critical commentary and discussion**\n\nThroughout this notebook, a VGG16 network with a custom classifier has been implemented with different metrics that allow us to know in depth the quality of the proposed solution. The use of the different parameters is not the result of chance, but of a process of rigorous experimentation and problem-solving. Thus, the effect of the following parameters on the network has been tested. *(All of the following metrics shown are AUC)*.\n\n* **Pre-trained model**: Pre-trained model previously trained on a large dataset on a large-scale image-classification task. **In this notebook the models tested were VGG16 and VGG19, obtaining the best overall with VGG16 on the ImageNet dataset (1.4 million labelled images and 1,000 different classes)**.\n\n* **Batch size**: The number of samples processed before the model updates its parameters. Thus, the smaller the batch size the faster convergence to optimal minima, since the model starts learning before having to see all the train data. It should be a “point” between 1 and the whole dataset batch size, that leads to the best generalization. Usually, a lower batch size will lead to better test accuracy, however, it will also cause more instabilities in validation depending on how noisy the data are in every batch. **In this notebook, the values tested were 16 - 32 - 64 - 128, obtaining the best overall result with 32.**\n\n* **Data augmentation.**  The use of data augmentation has not improved the network's performance. This may be due to the fact that the data augmentation is not good enough because, for example, it is too aggressive. Thus, when generating the variations of the existing data, the network does not generalise the problem properly and the predictions in tests are much worse. Therefore, we have tried to modify the data augmentation without obtaining the desired results. **Best overall result without data augmentation**.\n\n* **Loss function**: As part of the optimization algorithm, the error for the current state of the model must be estimated repeatedly. This requires the choice of an error function of the model so that the weights can be updated to reduce the loss on the next evaluation. **In this notebook, we use binary crossentropy**. Binary crossentropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions. The score is minimized and a perfect cross-entropy value is 0.\n\n* **Optimizer**: **It determines how the network will be updated based on the loss function**. Optimizers are algorithms used to change the attributes of the neural network such as weights and learning rate in order to reduce the losses. The chosen one based on experimentation on the problem is **RMSProp**, nevertheless, the ones tested in this notebook are \n\n    * **SGD** - Stochastic gradient descent. Classic method.\n    * **RMSProp** - Root Mean Square Propagation maintains per-parameter learning rates that are adapted based on the average of recent magnitudes of the gradients for the weight (e.g. how quickly it is changing). This means the algorithm does well on noisy problems.\n    * **Adam** - Extension of stochastic gradient descent. The name Adam is derived from adaptive moment estimation.  Empirical results demonstrate that Adam works well in practice and compares favourably to other stochastic optimization methods.\n\n* **Learning rate**: Learning rate determines the step size at each iteration while moving toward a minimum of a loss function. A value of 0,0005 is chosen in this notebook.\n\n* **Number of epochs**: Number of complete passes of the training data through the network. With the chosen set of parameters 50 achieve the best overall result.\n\n* **Structural multi-parametric MRI scans**: Fluid Attenuated Inversion Recovery (FLAIR), T1-weighted pre-contrast (T1w), T1-weighted post-contrast (T1Gd), T2-weighted (T2). **The best overall result was obtained with Fluid Attenuated Inversion Recovery (FLAIR) images**.\n\n\n* **Image criteria**: Between the collection of images found for a single patient, the considered criteria to select the images to be loaded are 'central', 'first' and 'all'. **The best overall result was obtained with the 'all' criteria**. The explanation for this is most likely related to the poorness of the dataset. The 'all' criteria takes a bigger amount of images although some of them are extremely poor quality ones.\n\n\n**Thus, in the end, the parameters that obtain the best results are: 32 Batch size, 0,0005 Learning rate, RMSProp optimizer, Binary crossentropy loss, No data augmentation, 50 epoch, Fluid Attenuated Inversion Recovery (FLAIR) scan and 'all' criteria. The best result obtained is an AUC of 0.802.**\n\n**This result is considered to be very good taking into account that in the challenge ranking it would have been placed in position 17 of 1556 participants, as can be observed from the public test data leaderboard: https://www.kaggle.com/competitions/rsna-miccai-brain-tumor-radiogenomic-classification/leaderboard?tab=public**","metadata":{}},{"cell_type":"markdown","source":"# **Future work. What have others done?**\n\nContrary to what top solutions look like, the winner’s final solution was one of the very first baselines he started with. There was no model ensembling, no complex/big models, and no sophisticated training techniques. So it’s very curious to know why some more complex ideas are not working. The winning model looks like this in the own words of the winner:\n\n- 3DCNN \n- Resnet10 \n- Binary Cross Entropy loss \n- Adam optimizer \n- 15 epochs \n- LR: epoch 1 to 10; lr = 0.0001 | epoch 10 to 15 lr=0.00005 \n- Batch size - 8\n- \"Best central image trick” to build 3D images. Each independent case has a different number of images for all the MRI scans. Using all the scans will confuse the model in learning the spatial dependence of the brain pixels that are not useful in our case. People took the central image (the image in the middle). Nevertheless, using the biggest image as a central image (the image that contains the largest brain cutaway view)  slightly improves the performance. \n\n**The result obtained by this proposal is an AUC of 0.621 with the private test set while the solution proposed in this notebook reaches 0.802 with the public test set. Thus, as they are different test sets, we cannot directly compare these results. However, we can say that the result obtained is very good as it is positioned in the top 20 of the results with the public test set among 1556 participants. Nevertheless, there is always room for improvement. Thus, the main lesson we can learn from the winner of the private test set is that it is important to properly treat the representation** we are passing to the model. We have to remember that each of the images of each patient are from different perspectives and, therefore, it is difficult for the model to understand the spatial dependencies in such a sparse dataset. Thus, the \"best central image trick\" used by the winner creates a 3D representation of the images that allows the model to understand the spatial relationships that exist in the brain scans.\n\nOn the other hand, other participants use more complex models, as is the case for the second winner (AUC of 0.618 in the private test set). This participant uses a **CNN-LSTM architecture that makes use of all 4 types of images simultaneously**. Thus obtaining more information.\n\nFinally, other participants, such as the third one (AUC of 0.617 in the private test set), employ transfer learning in a similar way to us but adding an **ensemble of each of the models obtained for each of the available image types.**\n\nIn summary, it is considered that **it is possible to improve the results by testing more different models that improve the spatial representations of the different images given the poor quality characteristics of the dataset.**\n","metadata":{}},{"cell_type":"markdown","source":"# **Annex: Results**\n\n\nResults-Brain-Tumor-Radiogenomic-Classification    \n    \n--------------- Test 1 ---------------    \n    \nNetwork = VGG16-128-64-32-1    \nEpochs = 7    \nBatch Size = 64    \nLR = 0.01    \npatient_mode = \"all\"    \nimage_mode = \"T2w\"    \noptimizer = \"RMSprop\"    \nloss_function = \"binary_crossentropy\"    \ndata_augmentation = False    \nsplit = 90-10-10    \nComment: Baseline    \n    \nAUC test: 0.653    \n    \n--------------- Test 2 ---------------    \n    \nNetwork = VGG16-128-64-32-1    \nEpochs = 7    \nBatch Size = 128    \nLR = 0.01    \npatient_mode = \"all\"    \nimage_mode = \"T2w\"    \noptimizer = \"RMSprop\"    \nloss_function = \"binary_crossentropy\"    \ndata_augmentation = False    \nsplit = 90-10-10    \nComment: Higher batch size    \n    \nAUC test: 0.645    \n    \n--------------- Test 3 ---------------    \n    \nNetwork = VGG16-128-64-32-1    \nEpochs = 7    \nBatch Size = 32    \nLR = 0.01    \npatient_mode = \"all\"    \nimage_mode = \"T2w\"    \noptimizer = \"RMSprop\"    \nloss_function = \"binary_crossentropy\"    \ndata_augmentation = False    \nsplit = 90-10-10    \nComment: Smaller batch size    \n    \nAUC test: 0.649    \n    \n    \n--------------- Test 4 ---------------    \n    \nNetwork = VGG16-128-64-32-1    \nEpochs = 15    \nBatch Size = 32    \nLR = 0.01    \npatient_mode = \"all\"    \nimage_mode = \"T2w\"    \noptimizer = \"RMSprop\"    \nloss_function = \"binary_crossentropy\"    \ndata_augmentation = False    \nsplit = 90-10-10    \nComment: Trained on more epochs    \n    \nAUC test: 0.664    \n    \n--------------- Test 5 ---------------    \n    \nNetwork = VGG16-128-64-32-1    \nEpochs = 25    \nBatch Size = 32    \nLR = 0.01    \npatient_mode = \"all\"    \nimage_mode = \"T2w\"    \noptimizer = \"RMSprop\"    \nloss_function = \"binary_crossentropy\"    \ndata_augmentation = False    \nsplit = 90-10-10    \nComment: Trained on more epochs    \n    \nAUC test: 0.666    \n    \n--------------- Test 6 ---------------    \n    \nNetwork = VGG16-128-64-32-1    \nEpochs = 25    \nBatch Size = 32    \nLR = 0.01    \npatient_mode = \"all\"    \nimage_mode = \"T2w\"    \noptimizer = \"Adam\"    \nloss_function = \"binary_crossentropy\"    \ndata_augmentation = False    \nsplit = 90-10-10    \nComment: Change optimizer    \n    \nAUC test: 0.5     \n    \n--------------- Test 7 ---------------    \n    \nNetwork = VGG16-128-64-32-1    \nEpochs = 25    \nBatch Size = 32    \nLR = 0.01    \npatient_mode = \"all\"    \nimage_mode = \"T2w\"    \noptimizer = \"SGD\"    \nloss_function = \"binary_crossentropy\"    \ndata_augmentation = False    \nsplit = 90-10-10    \nComment: Change optimizer    \n    \nAUC test: 0.624    \n    \n--------------- Test 8 ---------------    \n    \nNetwork = VGG16-128-64-32-16-1    \nEpochs = 25    \nBatch Size = 32    \nLR = 0.01    \npatient_mode = \"all\"    \nimage_mode = \"T2w\"    \noptimizer = \"RMSprop\"    \nloss_function = \"binary_crossentropy\"    \ndata_augmentation = False    \nsplit = 90-10-10    \nComment: Increase network depth    \n    \nAUC test: 0.667    \n    \n--------------- Test 9 ---------------    \n    \nNetwork = VGG16-256-128-64-32-16-1    \nEpochs = 25    \nBatch Size = 32    \nLR = 0.01    \npatient_mode = \"all\"    \nimage_mode = \"T2w\"    \noptimizer = \"RMSprop\"    \nloss_function = \"binary_crossentropy\"    \ndata_augmentation = False    \nsplit = 90-10-10    \nComment: Increase network depth    \n    \nAUC test: 0.666    \n    \n--------------- Test 10 ---------------    \n    \nNetwork = VGG19-128-64-32-16-1    \nEpochs = 25    \nBatch Size = 32    \nLR = 0.01    \npatient_mode = \"all\"    \nimage_mode = \"T2w\"    \noptimizer = \"RMSprop\"    \nloss_function = \"binary_crossentropy\"    \ndata_augmentation = False    \nsplit = 90-10-10    \nComment: VGG19    \n    \nAUC test: 0.5    \n    \n--------------- Test 11 ---------------    \n    \nNetwork = VGG16-128-64-32-16-1    \nEpochs = 25    \nBatch Size = 32    \nLR = 0.01    \npatient_mode = \"all\"    \nimage_mode = \"T2w\"    \noptimizer = \"RMSprop\"    \nloss_function = \"binary_crossentropy\"    \ndata_augmentation = True    \nsplit = 90-10-10    \nComment: Data augmentation    \n    \nAUC test: 0.5    \n    \n--------------- Test 12 ---------------    \n    \nNetwork = VGG16-128-64-32-16-1    \nEpochs = 25    \nBatch Size = 32    \nLR = 0.01    \npatient_mode = \"all\"    \nimage_mode = \"FLAIR\"    \noptimizer = \"RMSprop\"    \nloss_function = \"binary_crossentropy\"    \ndata_augmentation = False    \nsplit = 90-10-10    \nComment: Change type of images    \n    \nAUC test: 0.686    \n    \n--------------- Test 13 ---------------    \n    \nNetwork = VGG16-128-64-32-16-1    \nEpochs = 25    \nBatch Size = 32    \nLR = 0.01    \npatient_mode = \"all\"    \nimage_mode = \"T1w\"    \noptimizer = \"RMSprop\"    \nloss_function = \"binary_crossentropy\"    \ndata_augmentation = False    \nsplit = 90-10-10    \nComment: Change type of images    \n    \nAUC test: 0.5    \n    \n--------------- Test 14 ---------------    \n    \nNetwork = VGG16-128-64-32-16-1    \nEpochs = 25    \nBatch Size = 32    \nLR = 0.01    \npatient_mode = \"all\"    \nimage_mode = \"T1wCE\"    \noptimizer = \"RMSprop\"    \nloss_function = \"binary_crossentropy\"    \ndata_augmentation = False    \nsplit = 90-10-10    \nComment: Change type of images    \n    \nAUC test: 0.5    \n    \n--------------- Test 15 ---------------    \n    \nNetwork = VGG16-128-64-32-16-1    \nEpochs = 25    \nBatch Size = 16    \nLR = 0.01    \npatient_mode = \"all\"    \nimage_mode = \"FLAIR\"    \noptimizer = \"RMSprop\"    \nloss_function = \"binary_crossentropy\"    \ndata_augmentation = False    \nsplit = 90-10-10    \nComment: Lower batch size    \n    \nAUC test: 0.5    \n    \n--------------- Test 16 ---------------    \n    \nNetwork = VGG16-128-64-32-16-1    \nEpochs = 25    \nBatch Size = 32    \nLR = 0.01    \npatient_mode = \"central\"    \nimage_mode = \"FLAIR\"    \noptimizer = \"RMSprop\"    \nloss_function = \"binary_crossentropy\"    \ndata_augmentation = False    \nsplit = 90-10-10    \nComment: Change image mode    \n    \nAUC test: 0.5    \n    \n--------------- Test 17 ---------------    \n    \nNetwork = VGG16-128-64-32-16-1    \nEpochs = 25    \nBatch Size = 32    \nLR = 0.01    \npatient_mode = \"first\"    \nimage_mode = \"FLAIR\"    \noptimizer = \"RMSprop\"    \nloss_function = \"binary_crossentropy\"    \ndata_augmentation = False    \nsplit = 90-10-10    \nComment: Change image mode    \n    \nAUC test: 0.5    \n    \n--------------- Test 18 ---------------    \n    \nNetwork = VGG16-128-64-32-16-1    \nEpochs = 25    \nBatch Size = 32    \nLR = 0.005    \npatient_mode = \"all\"    \nimage_mode = \"FLAIR\"    \noptimizer = \"RMSprop\"    \nloss_function = \"binary_crossentropy\"    \ndata_augmentation = False    \nsplit = 90-10-10    \nComment: Lower learning rate    \n    \nAUC test: 0.696    \n    \n--------------- Test 19 ---------------    \n    \nNetwork = VGG16-128-64-32-16-1    \nEpochs = 30    \nBatch Size = 32    \nLR = 0.001    \npatient_mode = \"all\"    \nimage_mode = \"FLAIR\"    \noptimizer = \"RMSprop\"    \nloss_function = \"binary_crossentropy\"    \ndata_augmentation = False    \nsplit = 90-10-10    \nComment: Lower learning rate + more epochs    \n    \nAUC test: 0.761    \n    \n--------------- Test 20 ---------------    \n    \nNetwork = VGG16-128-64-32-16-1    \nEpochs = 50    \nBatch Size = 32    \nLR = 0.001    \npatient_mode = \"all\"    \nimage_mode = \"FLAIR\"    \noptimizer = \"RMSprop\"    \nloss_function = \"binary_crossentropy\"    \ndata_augmentation = False    \nsplit = 90-10-10    \nComment: More epochs    \n    \nAUC test: 0.773    \n    \n--------------- Test 21 ---------------    \n    \nNetwork = VGG16-128-64-32-16-1    \nEpochs = 40    \nBatch Size = 32    \nLR = 0.0005    \npatient_mode = \"all\"    \nimage_mode = \"FLAIR\"    \noptimizer = \"RMSprop\"    \nloss_function = \"binary_crossentropy\"    \ndata_augmentation = False    \nsplit = 90-10-10    \nComment: Lower learning rate  + less epochs    \n    \nAUC test: 0.779    \n    \n**--------------- Test 22 ---------------**    \n\n**Network = VGG16-128-64-32-16-1    \nEpochs = 50    \nBatch Size = 32    \nLR = 0.0005    \npatient_mode = \"all\"    \nimage_mode = \"FLAIR\"    \noptimizer = \"RMSprop\"    \nloss_function = \"binary_crossentropy\"    \ndata_augmentation = False    \nsplit = 90-10-10    \nComment: More epochs**    \n    \n**AUC test: 0.802** ","metadata":{"_kg_hide-input":false}}]}